---
title: "Financial Analytics - Trabalho Final"
authors:
  - name: Michel Maurice Conjaud
  - name: Hélio Pereira Oliveira
  - name: Renan Cabral
  - name: Tiago Evangelista Pardo
format:
  html:
    code-fold: true
    embed-resources: true
    smooth-scroll: true
    theme: cerulean
    toc: true
    toc-expand: true
    toc-title: "Sumário"
    toc_float: true

execute:
  freeze: true
  warning: false
  cache: true
---

# 0. Bibliotecas e Importação de Dados

Bibliotecas
```{r bibliotecas}
#| output: false
library(ggplot2)
library(fpp3)
library(rugarch)
library(tsibble)
library(yfR)
library(zoo)
library(xts)
library(lubridate)
library(patchwork)
library(lmtest)
library(fGarch)
library(xts)
library(FinTS)
library(tidyquant)
library(plotly)
library(timetk)

```

Importação das bases de dados
* **Ativos:** Nvidia, Banco Santander, Amazon, Localiza, Prio, Taurus
* **Data de início:** 2019-11-01
* **Data final:** 24/06/2024

```{r importacao dados}
#| output: false

data_inicio <- '2019-01-01'
 

ativos <- c(
  "NVDC34.SA",
  "BCSA34.SA", 
  "AMZO34.SA",
  "RENT3.SA",
  "PRIO3.SA",
  "TASA4.SA"
)
 

# Importando base da API do Yahoo

#| output: false

da <- yfR::yf_get(
    ativos,
    first_date = data_inicio,
    last_date = Sys.Date(),
    bench_ticker = "^BVSP",
    type_return = "log",
    freq_data = "daily",
    do_complete_data = TRUE
)    %>% 
    select( ticker, ref_date, price_adjusted, ret_closing_prices )
 
        
# Capturando menor data comum entre as empresas do portfólio
data_corte <- da |>
  dplyr::group_by(ticker) |>
  dplyr::filter(ref_date == min(ref_date)) |>
  dplyr::ungroup() |>
  with(max(ref_date))

# Filtrando base com data mínima
da_train <- da |>
  dplyr::filter(ref_date > data_corte) 

# Transformando a base em Tsibble
da_tsibble <- da_train |>
  as_tsibble(key = ticker, index = ref_date, regular = FALSE)

# # Importando base com retornos do Ibovespa em Log 
# retornos_mercado_log  <- yfR::yf_get(
#     '^BVSP',
#     first_date = data_corte,
#     last_date = Sys.Date(),
#     bench_ticker = "^BVSP",
#     type_return = "log",
#     freq_data = "daily",
#     do_complete_data = TRUE
#     )  %>% 
#     select(ref_date, ret_closing_prices)  %>%
#     filter(ref_date > data_corte)  %>%  
#     na.omit()  %>% 
#     distinct(ref_date, .keep_all = TRUE)

# Importando base com retornos do Ibovespa em Arit 
retornos_mercado_arit  <- yfR::yf_get(
    '^BVSP',
    first_date = data_corte + 1,
    last_date = Sys.Date(),
    bench_ticker = "^BVSP",
    type_return = "arit",
    freq_data = "daily",
    do_complete_data = TRUE
    )  %>% 
    select(ref_date, ibov = ret_closing_prices)  %>%
    filter(ref_date > data_corte)  %>%  
    na.omit()  %>% 
    distinct(ref_date, .keep_all = TRUE)

# Importando base com retornos da carteira em Arit 
retornos_carteira_arit <- yfR::yf_get(
    ativos,
    first_date = data_corte,
    last_date = Sys.Date(),
    bench_ticker = "^BVSP",
    type_return = "arit",
    freq_data = "daily",
    do_complete_data = TRUE
    ) %>% 
    select( ref_date, ret_closing_prices )  %>% 
    na.omit()
```

# 1. Gráficos de Preço, Retorno e Avaliação de Heterodasticidade Condicional

## 1.1. Gráfico de preços
```{r funcao legenda}
#Função para formatar legenda
quarter_label <- function(x) {
  paste0(year(x), "Q", quarter(x))
}  
```

```{r serie preco}
#| fig-width: 7
#| fig-height: 10

#criando gráfico da série de preços
da_tsibble |>
  autoplot(price_adjusted, colour = "black") +
  facet_wrap(~ticker, scales = "free_y", ncol = 1)+
  scale_x_date(date_breaks = "4 months", label=quarter_label) +
  labs ( title = "Série de Preço", y = "Preço", x = "Data") +
 
  theme(
    plot.title = element_text(hjust = .5),
    axis.text.x = element_text(hjust = 1, angle = 45)
  )
 
```  

## 1.2. Log Retornos
```{r serie log retornos}
#| fig-width: 7
#| fig-height: 10
#| 
#criando gráfico da série de retornos
da_tsibble |>
  autoplot(ret_closing_prices, colour = "black") +
  facet_wrap(~ticker, scales = "free_y", ncol = 1) +
  scale_x_date(date_breaks = "3 months", labels = quarter_label) +
  labs ( title = "Série de Log Retornos", y = "Log Retornos", x = "Data") +
  theme(plot.title = element_text(hjust = 0.5),
    axis.text.x = element_text(hjust = 1, angle = 45)
  )

```

## 1.3 Avaliando heterocedasticidade condicional

Criação do log retorno ao quadrado (proxi da volatilidade)

```{r plot dos retornos ao quadrado}
#| fig-width: 7
#| fig-height: 10

da_tsibble |>
  dplyr::mutate(ret2 = ret_closing_prices^2) |>
  autoplot(ret2, colour = "black") +
  facet_wrap(~ticker, ncol = 1)

```

**Verificando a existência de heterodasticidade condicional através dos seguintes métodos:**

* 1.2.1. ACF dos retornos quadráticos

* 1.2.2. Teste Ljung-Box dos retornos quadráticos

* 1.2.3. Teste Langrange Multiplier

### 1.2.1. ACF dos retornos ao quadrado

```{r criando ret quadrado}
# Gerando variável dos retornos quadráticos
da_tsibble  <-  da_tsibble |>
  dplyr::mutate(ret2 = ret_closing_prices^2)
```

```{r acf retornos ao quadrado}
#| fig-width: 7
#| fig-height: 10

#ACF dos retornos ao quadrado
da_tsibble |>
  ACF(ret2) |>
  autoplot()
```

**Interpretação**
* AMZO3, BCSA3, PRIO3 e RENT3 mostram autocorrelações significativas nos primeiros lags, indicando que a volatilidade desses retornos pode ser dependente no tempo. 

* NVCD, por outro lado, não apresenta uma dependência temporal forte, sugerindo ausência de heterocedasticidade significativa. 

* TASA4 tem algumas autocorrelações significativas, mas menos pronunciadas em comparação com as outras séries.

### 1.2.2. Teste Ljung-Box
```{r teste ljung box}

resultados <- data.frame(Empresa = character(), "P-Valor" = numeric())

for (empresa in ativos) {
  # Filtrar o dataframe para a empresa atual
  da_tsibble_filtered <- da_tsibble |> filter(ticker == empresa)
  
  # Realizar o teste Box-Ljung
  box_test_result <- Box.test(da_tsibble_filtered$ret2, type = "Ljung-Box")
  
  # Adicionar o resultado ao dataframe de resultados
  resultados <- rbind(resultados, 
                               data.frame(Empresa = empresa, 
                                          "P-Valor" = box_test_result$p.value 
                                          ))
}

print(resultados)
```

**Interpretação**
* Não rejeita a hipótese nula: NVDC34.SA

* Rejeitam a hipótese nula: BCSA34.SA, RENT3.SA, PRIO3.SA e TASA4.SA, AMZO34.SA

### 1.2.3. Teste Lagrange Multiplier
```{r teste lm}


resultados_lm <- data.frame(Empresa = character(), "P-Valor" = numeric())

for (empresa in ativos) {
  # Filtrar o dataframe para a empresa atual
  da_tsibble_filtered <- da_tsibble |> filter(ticker == empresa)

  teste_lm <- ArchTest(da_tsibble_filtered$ret_closing_prices)

  # Adicionar o resultado ao dataframe de resultados
  resultados_lm <- rbind(resultados_lm, 
                               data.frame(Empresa = empresa, 
                                          "P-Valor" = teste_lm$p.value 
                                          ))
}

print(resultados_lm)
```

**Interpretação**
* Não rejeita a hipótese nula: NVDC34.SA

* Rejeitam a hipótese nula: BCSA34.SA, AMZO34.SA, RENT3.SA, PRIO3.SA, e TASA4.SA

# 2. Ajustando modelos Arch / Garch

## 2.1. Ajuste

```{r modelo garch para cada empresa}
# Lista para comparação de modelos
modelos_garch <- list()
ics_modelos_garch  <- list()
resultados_tidy <- list()

distribuicao_erro  <- c("norm", "std")

for (empresa in unique(da_tsibble$ticker)) {
  retornos <- da_tsibble %>%
  subset(ticker== empresa) %>%
  select(ref_date,ret_closing_prices)
ret <- xts(retornos[,-1],order.by = ymd(retornos$ref_date))[-1,]
ret2 = ret**2
  # Loop pelos parâmetros de tipo de distribuição e m e n
  for (distribuicao in distribuicao_erro) {
    for (m in 1:3) {
      for (n in 0:3) {
      
        # Definir a fórmula do modelo com os parâmetros m e n atuais
        formula_garch <- paste0("~garch(", m, ",", n, ")")
        # Ajustar o modelo GARCH
        modelo_atual <- garchFit(formula = as.formula(formula_garch), data = ret, trace = FALSE, 
                                 include.mean = TRUE, cond.dist = distribuicao)
        # Armazenar o modelo ajustado na lista para visualização detalhada
        modelos_garch[[paste0("garch_", m, "_", n, "_", distribuicao, "_", empresa)]] <- modelo_atual
        # Armazenar o modelo ajustado na lista para comparação dos critérios de informação
        ics_modelos_garch[[paste0("garch_", m, "_", n, "_", distribuicao, "_", empresa)]] <- modelo_atual@fit$ics
    }
  }
}
  # Formatando resultados para melhor visualização 
  for (nome_modelo in names(ics_modelos_garch)) {
    valores <- ics_modelos_garch[[nome_modelo]]
    df <- data.frame(
      AIC = valores["AIC"],
      BIC = valores["BIC"]
    )
      resultados_tidy[[nome_modelo]] <- df
  }
} 

resultados_tidy_df <- do.call(rbind, resultados_tidy) %>% 
  rownames_to_column()  %>% 
    separate(rowname, c("Modelo", "m", "n", "dist", "ticker"),
    sep = "_")  %>% 
    arrange(AIC)  %>% 
    distinct(ticker, .keep_all = TRUE)

resultados_tidy_df
```

## 2.2. Análise dos resíduos
```{r analise dos residuos}

residuos <- list()
par(mfrow = c(6, 1))

# Iterando sobre cada linha do data frame `resultados_tidy_df`

for (i in 1:nrow(resultados_tidy_df)) {
  
  # Concatenando o nome do modelo 
  model_name <- paste0(resultados_tidy_df$Modelo[i], "_", 
                       resultados_tidy_df$m[i], "_", 
                       resultados_tidy_df$n[i], "_", 
                       resultados_tidy_df$dist[i], "_", 
                       resultados_tidy_df$ticker[i])

  
  # Checando se o modelo está entre os melhores
  if (model_name %in% names(modelos_garch)) {
    # Acessando os melhores modelos e calculando os resíduos
    current_model <- modelos_garch[[model_name]]
    residuos[[model_name]] <- residuals(current_model, standardize=TRUE)
    
    # Plotando os resíduos
    ts.plot(residuos[[model_name]], main = model_name)
  }
}


=======

```

**Interpretação**
* Padrões: Não foi encontrado padrões claros, o que é um bom sinal. Isso sugere que os modelos estão capturando bem a estrutura dos dados.
* Outliers: Todos os gráficos têm alguns outliers, o que faz sentido em séries financeiras. 
* Heterocedasticidade: três ações mostram variações na amplitude dos resíduos, sugerindo que pode haver alguma heterocedasticidade residual (RENT3.SA & NVDC334.SA & BCSA34.SA). 


# 3. Previsão dos Ativos
```{r}

forecast_amazon <- predict(modelos_garch$garch_1_2_std_AMZO34.SA, n.ahead = 1)
forecast_santander <- predict(modelos_garch$garch_1_1_std_BCSA34.SA, n.ahead = 1)
forecast_nvidia <- predict(modelos_garch$garch_1_1_std_NVDC34.SA, n.ahead = 1)
forecast_localiza <- predict(modelos_garch$garch_2_1_std_RENT3.SA, n.ahead = 1)
forecast_prio <- predict(modelos_garch$garch_1_1_std_PRIO3.SA, n.ahead = 1)
forecast_taurus <- predict(modelos_garch$garch_1_1_std_TASA4.SA, n.ahead = 1)


# Criar um vetor com as previsões de volatilidade para cada ativo
vetor_previsoes <- c(
  forecast_nvidia$standardDeviation,
  forecast_santander$standardDeviation,
  forecast_amazon$standardDeviation,
  forecast_localiza$standardDeviation,
  forecast_prio$standardDeviation,
  forecast_taurus$standardDeviation
)

# Criar um dataframe com os nomes dos ativos
forecast_ativos <- data.frame(
  ativo = c("AMZO34.SA", "BCSA34.SA", "NVDC34.SA", "RENT3.SA", "PRIO3.SA", "TASA4.SA"),
  serie = vetor_previsoes
)

# Exibir o dataframe criado
print(forecast_ativos)

# 3. Previsão da volatilidade dos Ativos

```

## 4.0 Comparar volatilidades entre os retornos selecionados (quais são maiores e menores, relacionando com algum storytelling);

* Maior Volatilidade: Amazon possui a maior volatilidade, refletindo seu alto nível de exposição ao mercado global e inovações tecnológicas.

* Menor Volatilidade: Banco Santander, Taurus, Nvidia, e Prio apresentam menor volatilidade, sugerindo operações estáveis e previsíveis em seus respectivos setores.

* Volatilidade Intermediária: Localiza tem uma volatilidade moderada, compatível com o setor de mobilidade que combina demanda estável com variações econômicas.

* Obs: NVIDIA está surfando o boom de IA, alcançando os USDD3Tri de valor de mercado, tendência de continuar subida de preço. 
* Obs: Santander, bancos tendem a ser mais estáveis no mercado brasileiro. 

## 4.2. Modelagem Garch do Portfolio
```{r modelagem garch portfolio}
modelos_garch_portfolio <- list()
ics_modelos_garch_portfolio  <- list()
resultados_tidy_portfolio <- list()


for (distribuicao in distribuicao_erro) {
  for (m in 1:3) {
    for (n in 0:3) {
    
      # Definir a fórmula do modelo com os parâmetros m e n atuais
      formula_garch <- paste0("~garch(", m, ",", n, ")")

      # Ajustar o modelo GARCH
      modelo_atual <- garchFit(
        formula = as.formula(formula_garch),
        data = retorno_ponderado_portfolio$retornos,
        trace = FALSE,          
        include.mean = TRUE, 
        cond.dist = distribuicao)

      # Armazenar o modelo ajustado na lista para visualização detalhada
      modelos_garch_portfolio[[paste0("garch_", m, "_", n, "_", distribuicao)]] <- modelo_atual
      
      # Armazenar o modelo ajustado na lista para comparação dos critérios de informação
      ics_modelos_garch_portfolio[[paste0("garch_", m, "_", n, "_", distribuicao)]] <- modelo_atual@fit$ics
    }
  }
}
# Formatando resultados para melhor visualização 
  for (nome_modelo in names(ics_modelos_garch_portfolio)) {
    valores <- ics_modelos_garch_portfolio[[nome_modelo]]
    df <- data.frame(
      AIC = valores["AIC"],
      BIC = valores["BIC"]
    )
      resultados_tidy_portfolio[[nome_modelo]] <- df
}

resultados_tidy_df_portfolio <- do.call(rbind, resultados_tidy_portfolio)%>% 
  rownames_to_column()  %>% 

    arrange(AIC)  %>% 
    slice(1)

resultados_tidy_df_portfolio
```

## 4.3. Forecast do Portfólio
```{r forecast portfolio}
forecast_volatilidade_portifolio <- predict(modelos_garch_portfolio$garch_1_1_std, n.ahead = 1)
forecast_volatilidade_portifolio
```

# Otimização do Portfólio
formatando para wide e transformando em formato objeto xts
```{r}
 
portfolio_wide_xts  <-  portfolio %>% 
  spread(ticker, value = ret_closing_prices)  %>% 
    tk_xts()
# Retirando coluna de data
portfolio_wide_xts <- portfolio_wide_xts[,-7]
# Convertendo colunas para numéricas
portfolio_wide_xts <- apply(portfolio_wide_xts, 2, as.numeric)
 
```

### Matriz covariância
```{r matriz covariancia}
cov_mat <- cov(portfolio_wide_xts) * 252
print(round(cov_mat,4))
```

### Media de retornos
```{r media de retornos portfolio }
media_retornos  <- colMeans(portfolio_wide_xts)
media_retornos
``` 

### Risco (Desvio Padrão)
```{r risco portfolio}
risco_portfolio  <- sqrt(t(pesos) %*% (cov_mat %*% pesos))
```

### Sharpe ratio
```{r sharpe ratio}
sharpe_ratio  <- media_retornos / risco_portfolio
```

```{r otimizacao portfolio}

numero_simulacoes_portfolio = 5000

simulacao_retornos <- function(i) {
  pesos <- runif(length(ativos))
  pesos <- pesos / sum(pesos)
  retorno_portfolio <- sum(pesos * media_retornos)
  desvio_padrao_portfolio <- as.numeric(sqrt(t(pesos) %*% (cov_mat %*% pesos)))
  sr <- retorno_portfolio / desvio_padrao_portfolio

  pesos |>
    purrr::set_names(ativos) |>
    tibble::enframe() |>
    tidyr::pivot_wider() |>
    dplyr::mutate(
      return = retorno_portfolio,
      risk = desvio_padrao_portfolio,
      sharpe = sr
    )
}

portfolio_values <- purrr::map(1:2000, simulacao_retornos, .progress = TRUE) |>
  bind_rows(.id = "run")

min_var <- portfolio_values[which.min(portfolio_values$risk),]
max_sr <- portfolio_values[which.max(portfolio_values$sharpe),]

```

Gráfico de mínimo Value at Risk
```{r grafico min var}

min_var |>
  pivot_longer(2:7) |>
  mutate(name = forcats::fct_reorder(name, value)) |>
  ggplot(aes(name, value)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Ativos",
    y = "Pesos",
    title = "Portfólio de mínima variância "
  )
```

Gráfico de máximo Value at Risk
```{r grafico max sr}
max_sr |>
  pivot_longer(2:7) |>
  mutate(name = forcats::fct_reorder(name, value)) |>
  ggplot(aes(name, value)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  labs(
    x = "Ativos",
    y = "Pesos",
    title = "Tangency portfolio weights"
  )
```

Gráfico otimização
```{r grafico retornos risco}
portfolio_values |>
  ggplot(aes(x = risk, y = return, color = sharpe)) +
  geom_point() +
  theme_classic() +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    x = 'Risk',
    y = 'Returns',
    title = "Otimização de Portfólio e Fronteira Eficiente"
  ) +
  geom_point(
    aes(x = risk, y = return),
    data = min_var,
    color = 'green',
    size = 3
  ) +
  geom_point(
    aes(x = risk, y = return),
    data = max_sr,
    color = 'orange',
    size = 3
  )
```


## VaR do portfolio
```{r}
pesos_finais <- min_var |>
  dplyr::select(2:7) |>
  as.numeric()

rt_final <- mean(forecast_ativos$serie * pesos_finais)
st_dev_final <- sqrt(pesos_finais %*% cov_mat %*% pesos_finais)
nu <- min(vol_futuro$shape)
valor_t <- qt(.95, nu)

(VaR <- rt_final + valor_t * st_dev_final / sqrt(nu/(nu-2)))
```

# CAPM
```{r}

retornos_portfolio_mercado <- retornos_mercado_arit |>
  dplyr::inner_join(retorno_ponderado_portfolio, "ref_date") |>
  tidyr::drop_na()

(beta_geral <- with(retornos_portfolio_mercado, cov(retornos, ibov) / var(ibov)))

calcular_beta <- function(ativo) {
  da_tsibble |>
    dplyr::filter(ticker == ativo) |>
    dplyr::inner_join(retornos_mercado_arit, "ref_date") |>
    tidyr::drop_na() |>
    with(cov(ret_closing_prices, ibov) / var(ibov))
}

betas <- purrr::map_dbl(ativos, calcular_beta) |>
  purrr::set_names(ativos)

sum(betas * pesos_finais)
beta_geral

```

```{r}

capm_lm_tudo <- lm(retornos ~ ibov, data = retornos_portfolio_mercado) |>
  broom::tidy() |>
  dplyr::filter(term == "ibov") |>
  with(estimate)

capm_lm_individual <- purrr::map_dbl(ativos, \(ativo) {
  da_model <- da_tsibble |>
    dplyr::filter(ticker == ativo) |>
    dplyr::inner_join(retornos_mercado_arit, "ref_date")
  lm(ret_closing_prices ~ ibov, data = da_model) |>
    broom::tidy() |>
    dplyr::filter(term == "ibov") |>
    dplyr::pull(estimate)
}) |>
  purrr::set_names(ativos)

capm_lm_tudo

capm_lm_individual
```


1. Calcular retorno da carteira otimizada
2. Comparar retornos <- ?
3. Arrumar CAPM
4. Interpretar CAPM
5. Matriz de covariância <- ?